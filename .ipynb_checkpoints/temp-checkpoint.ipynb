{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Activation Functions\n",
    "'''\n",
    "\n",
    "class Sigmoid:\n",
    "    def evaluate(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def gradient(self,x):\n",
    "        S = self.evaluate(x)\n",
    "        return S*(1-S)\n",
    "    \n",
    "class ReLU:\n",
    "    def evaluate(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return x>= 0\n",
    "        \n",
    "class Softmax:\n",
    "    def evaluate(self, x):\n",
    "        x = x - np.max(x)\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def gradient(self,x):\n",
    "        n = len(x)\n",
    "        S = self.evaluate(x)\n",
    "        grad = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(i):\n",
    "                grad[i,j] = -S[j]*S[i]\n",
    "                grad[j,i] = grad[i,j]\n",
    "            grad[i,i] = S[i]*(1-S[i])\n",
    "        return grad\n",
    "    \n",
    "class NormalizedSoftmax:\n",
    "    def evaluate(self, x):\n",
    "        x = x/np.max(x)\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def gradient(self,x):\n",
    "        n = len(x)\n",
    "        S = self.evaluate(x)\n",
    "        grad = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(i):\n",
    "                grad[i,j] = -S[j]*S[i]\n",
    "                grad[j,i] = grad[i,j]\n",
    "            grad[i,i] = S[i]*(1-S[i])\n",
    "        return grad    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Functions\n",
    "'''\n",
    "        \n",
    "class SingleCrossEntropy:\n",
    "    # Note: y needs to be encoded as classes 0,1,2,...\n",
    "    def evaluate(self, y_hat, y):\n",
    "        if y == 1:\n",
    "            return -np.log(y_hat)\n",
    "        else:\n",
    "            return -np.log(1-y_hat)\n",
    "    \n",
    "    def gradient(self, y_hat, y):\n",
    "        if y == 1:\n",
    "            return -1/y_hat\n",
    "        else:\n",
    "            return 1/(1-y_hat)\n",
    "    \n",
    "class MultiCrossEntropy:\n",
    "    # Note: y needs to be encoded as classes 0,1,2,...\n",
    "    def evaluate(self, y_hat, y):\n",
    "        return -np.log(y_hat[y])\n",
    "    \n",
    "    def gradient(self, y_hat, y):\n",
    "        grad = np.zeros(len(y_hat))\n",
    "        grad[y] = -1/y_hat[y]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimisation Algorithms\n",
    "'''\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, loss, epochs = 1, rate = 0.1):\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        if not isinstance(rate, list):\n",
    "            self.rate = [rate]*self.epochs\n",
    "        else:\n",
    "            self.rate = rate\n",
    "        \n",
    "    def train(self, nn, X, Y):\n",
    "        ind = list(range(X.shape[0]))\n",
    "        for e in tqdm(range(self.epochs)):\n",
    "            random.shuffle(ind)\n",
    "            for i in ind:\n",
    "                nn.FwdProp(X[i])\n",
    "                nn.BackProp(Y[i], self.loss)\n",
    "                for j in range(nn.n_layers - 1):\n",
    "                    nn.layers[1+j].weights -= self.rate[e]*nn.delta_ws[j]\n",
    "                    nn.layers[1+j].bias -= self.rate[e]*nn.delta_bs[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Layers\n",
    "\n",
    "TODO: Make no activation for input, Make convolutional layers\n",
    "'''\n",
    "class DenseLayer:\n",
    "    def __init__(self, activation, n_nodes, is_input = False):\n",
    "        self.activation = activation\n",
    "        self.n_nodes = n_nodes\n",
    "        self.is_input = is_input\n",
    "        \n",
    "    def FwdPropOutput(self, prev):\n",
    "        if not self.is_input:\n",
    "            self.z = np.matmul(self.weights, prev) + self.bias\n",
    "            self.a = self.activation.evaluate(self.z)\n",
    "        else:\n",
    "            self.a = prev\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Fix predict, it currently needs to take in more than 1 input\n",
    "      Fix evaluate wrong shape\n",
    "      Include regularistion\n",
    "'''\n",
    "class Network:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        for i in range(1,self.n_layers):\n",
    "            this_length = self.layers[i].n_nodes\n",
    "            prev_length = self.layers[i-1].n_nodes\n",
    "            self.layers[i].weights = np.random.normal(0.1,0.5, this_length*prev_length).reshape(this_length, prev_length)\n",
    "            self.layers[i].bias = np.random.normal(0.1,0.5,this_length)\n",
    "            \n",
    "    def FwdProp(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.FwdPropOutput(x)\n",
    "        return x\n",
    "    \n",
    "    def BackProp(self, y, loss):\n",
    "        deltas = [None]*(self.n_layers-1)\n",
    "        delta_ws = [None]*(self.n_layers-1)\n",
    "        delta_bs = [None]*(self.n_layers-1)\n",
    "\n",
    "        deltas[-1] = np.matmul(loss.gradient(self.layers[-1].a, y), self.layers[-1].activation.gradient(self.layers[-1].z))\n",
    "        delta_ws[-1] = np.matmul(deltas[-1].reshape(-1,1), self.layers[-2].a.reshape(1,-1))\n",
    "        delta_bs[-1] = deltas[-1]\n",
    "        \n",
    "        for i in range(2, self.n_layers):\n",
    "            deltas[-i] = self.layers[-i].activation.gradient(self.layers[-i].z)*np.matmul(deltas[1-i].reshape(1,-1), self.layers[1-i].weights)\n",
    "            delta_ws[-i] = np.matmul(deltas[-i].reshape(-1,1), self.layers[-i-1].a.reshape(1,-1))\n",
    "            delta_bs[-i] = deltas[-i].flatten()\n",
    "            \n",
    "        self.deltas = deltas\n",
    "        self.delta_ws = delta_ws\n",
    "        self.delta_bs = delta_bs\n",
    "        \n",
    "    def fit(self, X, Y, optimiser):\n",
    "        optimiser.train(self, X, Y)\n",
    "        \n",
    "    def evaluate(self, X, Y):\n",
    "        pred = np.array(self.predict(X)).flatten() ## FIX THIS\n",
    "        return sum(pred == Y)/len(Y)\n",
    "    \n",
    "    def evaluate_multi(self, X, Y):\n",
    "        pred = self.predict_multi(X)\n",
    "        return sum(pred == Y)/len(Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [np.round(self.FwdProp(X[i]),0) for i in range(X.shape[0])]\n",
    "    \n",
    "    def predict_multi(self, X):\n",
    "        return [np.argmax(self.FwdProp(X[i])) for i in range(X.shape[0])]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Problem 1: Simulated Lever Data\n",
    "\n",
    "We simulate the physical state of a lever (tilting to the left or right) given some left weight (lw) placed at a certain distance (ld) from the pivot and some right weight (rw) placed at a certain distance (rd) from the pivot. We will compare our NN implementation with a logistic regression as well as an identical architecture NN from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft = Softmax()\n",
    "relu = ReLU()\n",
    "norm_sft = NormalizedSoftmax()\n",
    "sig = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "ntrain = 6000\n",
    "lw = uniform.rvs(0,1, size = 10000, random_state = 1)\n",
    "rw = uniform.rvs(0,1, size = 10000, random_state = 2)\n",
    "ld = uniform.rvs(0,1, size = 10000, random_state = 3)\n",
    "rd = uniform.rvs(0,1, size = 10000, random_state = 4)\n",
    "tilt = np.sign(rw*rd-lw*ld)\n",
    "\n",
    "X = pd.DataFrame(dict(zip([\"lw\",\"rw\",\"ld\",\"rd\",\"tilt\"],[lw,rw,ld,rd,tilt])))\n",
    "X['tilt'], _ = pd.factorize(X['tilt'])\n",
    "X_train = X.loc[:ntrain,X.columns[:-1]].values\n",
    "Y_train = X.loc[:ntrain,X.columns[-1]].values\n",
    "X_test = X.loc[ntrain+1:,X.columns[:-1]].values\n",
    "Y_test = X.loc[ntrain+1:,X.columns[-1]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom NN Implementation on Lever Data\n",
    "\n",
    "We try two networks. Firstly we use no hidden layer and compare with sklearn logistic regression. Then we use a hidden layer and compare with the same architecture in Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom No Hidden Layer (Logistic Regression)\n",
    "Below we see with 50 epcochs we have about 91% accuracy on both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:13<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9080153307782036\n",
      "Test Accuracy:  0.9149787446861716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = DenseLayer(relu, 4, is_input = True)\n",
    "l2 = DenseLayer(sig, 1, is_input = False)\n",
    "nn = Network([l1,l2])\n",
    "l = SingleCrossEntropy()\n",
    "o = SGD(l, 50, 0.1)\n",
    "nn.fit(X_train,Y_train, o)\n",
    "print('Train Accuracy: ',nn.evaluate(X_train, Y_train))\n",
    "print('Test Accuracy: ',nn.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Logistic Regression\n",
    "Here we get around 91% on train and test again. Our implementation has the same level of performance as this well established module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acuuracy = 0.9085152474587569\n",
      "Test Accuracy = 0.9152288072018004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "mod = LogisticRegression(C=np.inf)\n",
    "mod.fit(X_train,Y_train.flatten())\n",
    "print('Train Acuuracy =', mod.score(X_train, Y_train))\n",
    "print('Test Accuracy =', mod.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Shallow Network\n",
    "We use a 10 node hidden layer with relu activation and run 50 epochs again. Below we achieve 97% on both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:21<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9731711381436428\n",
      "Test Accuracy:  0.9662415603900976\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO: Include runtime comparison\n",
    "'''\n",
    "\n",
    "l1 = DenseLayer(relu, 4, is_input = True)\n",
    "l2 = DenseLayer(relu, 10, is_input = False)\n",
    "l3 = DenseLayer(sig, 1, is_input = False)\n",
    "nn = Network([l1,l2,l3])\n",
    "l = SingleCrossEntropy()\n",
    "o = SGD(l, 50, 0.1)\n",
    "nn.fit(X_train,Y_train, o)\n",
    "print('Train Accuracy: ',nn.evaluate(X_train, Y_train))\n",
    "print('Test Accuracy: ',nn.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Shallow Network\n",
    "We use the same architecture as above built with keras sequential model. We run for 50 epochs again and get 91% accuracy on both train and test. This is significantly worse than the custom network approach. I'm not sure why this would be the case and intend to look into this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9086818695068359\n",
      "Test Accuracy:  0.914478600025177\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO: Look into performance worse than my custom\n",
    "'''\n",
    "import tensorflow as tf\n",
    "l1 = tf.keras.Input(shape = (4,))\n",
    "l2 = tf.keras.layers.Dense(units = 10, activation = 'relu')\n",
    "l3 = tf.keras.layers.Dense(units = 1, activation = 'sigmoid')\n",
    "model = tf.keras.Sequential([l1,l2,l3])\n",
    "model.compile(optimizer = 'sgd',  loss=\"binary_crossentropy\", metrics=\"acc\")\n",
    "model.fit(X_train, Y_train, epochs = 50, verbose = 0)\n",
    "print('Train Accuracy: ',model.evaluate(X_train, Y_train, verbose = 0)[1])\n",
    "print('Test Accuracy: ',model.evaluate(X_test, Y_test, verbose = 0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Problem 2: MNIST Handwriting Classification\n",
    "\n",
    "Now we want to try a multiclass classification problem. We will take the well known example of the MNIST handwriting classification dataset. Here we classify images of handwritten digits 0 to 9. See example observation below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = np.array([x.flatten() for x in train_X])[:10000]/255 # Take subset of training set for faster training\n",
    "test_X =  np.array([x.flatten() for x in test_X])[:2000]/255 # # Take subset of test set similarly\n",
    "train_y = train_y[:10000]\n",
    "test_y = test_y[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(3, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for i in range(len(grid)):\n",
    "    grid[i].imshow(train_X[i].reshape(28,28), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom NN Implementation on MNIST Data\n",
    "We will compare a shallow network from the custom implementation and keras sequential again. Later I hope to include convolutional layers, so such a comparison will then be included here too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Shallow Network\n",
    "We train for 10 epochs for the sake of train time. The performance does continue to improve for more epochs, so this is not max performance. With this amount of epochs we get 92% train and 85% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [06:19<00:00, 37.99s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO: Look into runtime, add regularistion\n",
    "'''\n",
    "l1 = DenseLayer(relu, 784, is_input = True)\n",
    "l2 = DenseLayer(relu, 256, is_input = False)\n",
    "l3 = DenseLayer(sft, 10, is_input = False)\n",
    "nn = Network([l1,l2,l3])\n",
    "l = MultiCrossEntropy()\n",
    "o = SGD(l, 10, 0.001)\n",
    "nn.fit(train_X,train_y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9198\n",
      "Test Accuracy:  0.8505\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', nn.evaluate_multi(train_X, train_y))\n",
    "print('Test Accuracy: ', nn.evaluate_multi(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Shallow Network\n",
    "With the same architecture and training epochs we get 92% train and 89% test. i.e. Roughly same train accuracy but a more noticeable difference in generalisation. Note also that training time is much faster in tensorflow (as expected). It will be interesting to compare for more epochs, as we are not at max performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9247999787330627\n",
      "Test Accuracy:  0.8865000009536743\n"
     ]
    }
   ],
   "source": [
    "# labels need to be one hot encoded\n",
    "temp = np.zeros((train_y.size, train_y.max()+1))\n",
    "temp[np.arange(train_y.size),train_y] = 1\n",
    "train_y = temp\n",
    "\n",
    "temp = np.zeros((test_y.size, test_y.max()+1))\n",
    "temp[np.arange(test_y.size),test_y] = 1\n",
    "test_y = temp\n",
    "\n",
    "l1 = tf.keras.Input(shape = (784,))\n",
    "l2 = tf.keras.layers.Dense(units = 256, activation = 'relu')\n",
    "l3 = tf.keras.layers.Dense(units = 10, activation = 'softmax')\n",
    "model = tf.keras.Sequential([l1,l2,l3])\n",
    "model.compile(optimizer = 'sgd',  loss=\"categorical_crossentropy\", metrics=\"acc\")\n",
    "model.fit(train_X, train_y, epochs = 10, verbose = 0)\n",
    "print('Train Accuracy: ', model.evaluate(train_X, train_y, verbose = 0)[1])\n",
    "print('Test Accuracy: ', model.evaluate(test_X, test_y, verbose = 0)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
